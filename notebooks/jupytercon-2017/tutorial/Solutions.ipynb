{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"POS-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "def return_documents():\n",
    "    \"\"\"\n",
    "    Returns a list of documents, where each document is a string\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    dataset = fetch_20newsgroups()\n",
    "    corpus = dataset.data\n",
    "    return corpus\n",
    "\n",
    "def tokenize_and_tag_documents(documents, nlp, sep_char=\"||||\"):\n",
    "    \"\"\"\n",
    "    Returns a list of lists of tokens. Each token has been \n",
    "    concatenated with its part of speech\n",
    "    \"\"\"\n",
    "    for doc in nlp.pipe(documents, parse=False, entity=False):\n",
    "        yield [word.lower_ + sep_char + word.pos_ for word in doc]\n",
    "        \n",
    "\n",
    "def build_model(tokenized_docs):\n",
    "    \"\"\"\n",
    "    Returns a gensim Word2Vec model trained on our corpus\n",
    "    \"\"\"\n",
    "    return Word2Vec(list(tokenized_docs), size=128)\n",
    "\n",
    "\n",
    "documents = return_documents()\n",
    "tokenized_and_tagged_documents = tokenize_and_tag_documents(documents, nlp)\n",
    "model = build_model(tokenized_and_tagged_documents)\n",
    "\n",
    "\n",
    "def return_alts(word):\n",
    "    pattern = re.compile(\"^\" + word + \"\\|\\|\\|\\|\")\n",
    "    return list(filter(lambda x: bool(pattern.match(x)), model.wv.vocab.keys()))\n",
    "\n",
    "if model:\n",
    "    print(model.most_similar('real||||ADJ'))\n",
    "    print()\n",
    "    print(model.most_similar('real||||ADV'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"POS-2\">Solution for POS Exercise 2</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "class PerceptronClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.iter = 0\n",
    "\n",
    "    def fit(self, X, y, epochs=100):\n",
    "        \"\"\"Fits self.weights, self.biases \"\"\"\n",
    "        self.initialize(X)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            for row, label in zip(X, y):\n",
    "                prediction = self.predict(row)\n",
    "                self.update(prediction, label, row)\n",
    "                self.iter += 1\n",
    "            X, y = shuffle(X, y) #important to reshuffle to avoid getting trapped\n",
    "            \n",
    "    def initialize(self, X):\n",
    "        self.bias = 0\n",
    "        self.weights = np.zeros(X.shape[1])#[:, np.newaxis]\n",
    "                    \n",
    "    def update(self, prediction, label, row):\n",
    "        \"\"\"Updates weights and biases based on the ground truth label\n",
    "        and the row\"\"\"\n",
    "        \n",
    "        if prediction*label <= 0:\n",
    "            update = label *  row            \n",
    "            self.weights += update\n",
    "            self.bias += label\n",
    "        \n",
    "    def predict_score(self, x):\n",
    "        \"\"\"Generates scores of \"x\". Uses self.weights and self.bias\"\"\"\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\" Converts prediction scores to 1s ands 0s.\"\"\"\n",
    "        predictions = np.where(self.predict_score(x) > 0, 1, -1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"DEP-2\">Solution to Dependency Parsing Exercise 2</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy import attrs\n",
    "\n",
    "def merge_matches(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge(label=label, tag='NNP' if label else span.root.tag_)\n",
    "\n",
    "\n",
    "# DEFINE my_python_pattern        \n",
    "#this should be a list of dictionaries, where each dictionary is {TOKEN PROPERTY: TOKEN VALUE}\n",
    "#PROPERTIES are found in spacy.attrs, e.g. POS\n",
    "my_python_pattern = [{attrs.LOWER:'python'}]        \n",
    "\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add_entity(\"PYTHON\", on_match = merge_matches)\n",
    "matcher.add_pattern(\"PYTHON\", my_python_pattern, label='Python')\n",
    "nlp.pipeline = [nlp.tagger, nlp.parser, matcher, nlp.entity]\n",
    "\n",
    "equivalence_verbs = ['be']\n",
    "\n",
    "def get_all_properties_of_python(text):\n",
    "    \"\"\"Converts text to document, and extracts all relations that define python equivalences\"\"\"\n",
    "    doc = nlp(text)\n",
    "    python_properties = []\n",
    "    for relation in extract_relations(doc):\n",
    "        ### Add your code here to define my_relation\n",
    "        ### We Want to grab relations that:\n",
    "        ### A) have python has a subject\n",
    "        ### B) relate python with an equivalence verb (e.g. \"be\")\n",
    "        ### note to get normalized \"be\" from is, are, etc, use token.lemma_\n",
    "        \n",
    "        my_relation = None\n",
    "        \n",
    "        python_properties.append(my_relation)\n",
    "    return python_properties\n",
    "\n",
    "\n",
    "get_all_properties_of_python(page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
