{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "util/installers.ipynb:41: UserWarning: Installing skater\n",
      "  \"        !conda install tensorflow -y >> ~/install.log        \\n\",\n",
      "util/installers.ipynb:44: UserWarning: You may need to refresh the notebook.\n",
      "  \"    except:\\n\",\n"
     ]
    }
   ],
   "source": [
    "%run util/installers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief primer on using SpaCy:\n",
    "\n",
    "* Spacy Objects:\n",
    "    * Documents\n",
    "    * Tokens\n",
    "    * Spans\n",
    "* Each can contain metadata\n",
    "* Metadata is added through Language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome ADJ ROOT welcome\n",
      "to ADP prep to\n",
      "JupyterCon PROPN pobj jupytercon\n",
      "2017 NUM nummod 2017\n",
      "! PUNCT punct !\n",
      "\n",
      "<spacy.tagger.Tagger object at 0x7face9592048>\n",
      "<spacy.pipeline.DependencyParser object at 0x7facececc188>\n",
      "<spacy.matcher.Matcher object at 0x7faceceb2128>\n",
      "<spacy.pipeline.EntityRecognizer object at 0x7facececc778>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# creates a Language model\n",
    "if 'nlp' not in globals():\n",
    "    nlp = spacy.load('en')\n",
    "\n",
    "# language models are *callable*, returning Document objects:\n",
    "document = nlp(\"Welcome to JupyterCon 2017!\")\n",
    "\n",
    "# documents provide *metadata* about the text, including tokenization, parts of speech, etc...\n",
    "for token in document:\n",
    "    print(token, token.pos_, token.dep_, token.lemma_)\n",
    "    \n",
    "    \n",
    "# When we call language models, were calling a *list* of functions that operate on documents:\n",
    "print()\n",
    "for function in nlp.pipeline:\n",
    "    print(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"800\"\n",
       "            src=\"https://spacy.io/docs/usage/lightning-tour\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7facecea8da0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://spacy.io/docs/usage/lightning-tour', 800, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization and Sentence Segmentation: What is a lexical unit?\n",
    "What is a word? [Dictionary entries + alternate word forms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"https://en.wikipedia.org/wiki/Special:Random\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7facececb358>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://en.wikipedia.org/wiki/Special:Random', 800, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Regex, Deterministic Approach:\n",
    "\n",
    "\n",
    "### Intro to Regex:\n",
    "\n",
    "Regex involves defining a sequence of characters, character sequences, quantifiers and rules in order to find certain types of text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Characters:\n",
    "\n",
    "Most characters are simple; they match themselves:\n",
    "\n",
    "Pattern | Matches | Example\n",
    "--- | --- \n",
    "Jupyter | Instances of \"Jupyter\" | Welcome to **Jupyter**Con 2017!\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Other characters are special, such as [], which define **character sets**:\n",
    "\n",
    "Pattern | Matches | Example\n",
    "--- | --- \n",
    "[JCe] |  Single instances of the characters of \"J\", \"C\", \"e\" | W**e**lcom**e** to **J**upyt**e**r**C**on 2017!\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Or **quantifiers** \\*, ?, or +:\n",
    "\n",
    "Pattern | Matches | Example\n",
    "--- | --- \n",
    "e* | Zero or more \"e\", as many as possible | W**e**lcom**e** to Jupyt**e**rCon 2017!\n",
    "e? | Zero or ones instances of \"e\" | W**e**lcom**e** to Jupyt**e**rCon 2017!\n",
    "me+ | \"m\", then at least one \"e\", as many as possible | Welco**me** to JupyterCon 2017!\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "These quantifiers act on the pattern directly preceeding it.\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "What will this match?\n",
    "</center>\n",
    "\n",
    "Pattern | Example \n",
    "--- | --- \n",
    "re? | Welcome to JupyterCon 2017! \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The \\*, +, and ? quantifiers are **greedy**, they will match as much text as they can.\n",
    "\n",
    "Pattern | Matches | Example\n",
    "--- | --- \n",
    "[A-Z][a-z]+ | Match the longest sequence of letters beginning with a capital | **Welcome** to **JupyterCon** 2017!\n",
    "\n",
    "Adding \"?\" to a quantifier makes the search thrifty instead:\n",
    "\n",
    "Pattern | Matches | Example\n",
    "--- | --- \n",
    "[A-Z][a-z]+? | Match the shortest sequence of letters beginning with a capital | **We**lcome to **Ju**pyter**Co**n 2017!\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Characters like \\s, \\d, are special sequences, denoting groups of characters:\n",
    "\n",
    "Pattern | Matches | Example\n",
    "--- | --- \n",
    "\\s | Whitespace. | Welcome**_**to**_**JupyterCon**_**2017!\n",
    "\\d | Digits. | Welcome to JupyterCon **2017**!\n",
    "\n",
    "Adding an extra \"\\\" can escape special characters\n",
    "\n",
    "Pattern | Matches | Example\n",
    "--- | --- \n",
    "\\\\\\s | Instances of \"\\s\" | Welcome to JupyterCon 2017!\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Special characters can also restrict matches to certain positions:\n",
    "\n",
    "\n",
    "Pattern | Matches | Example\n",
    "--- | --- \n",
    "^W | Match \"W\" if string begins with \"W\" | **W**elcome to JupyterCon 2017!\n",
    "W$ | Match \"W\" if string ends with \"W\" | Welcome to JupyterCon 2017!\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We can also use \"|\" to match patternA or patternB, given patternA|patternB:\n",
    "\n",
    "Pattern | Matches | Example\n",
    "--- | --- \n",
    "to&#124;Jupyter| Match \"to\" or \"Jupyter\" | Welcome **to** **Jupyter**Con 2017!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_datascience": {}
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Welcome to JupyterCon 2017!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%matplotlib notebook\n",
    "from ipywidgets import Button, Textarea, Layout, Box, Label\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import re\n",
    "import html\n",
    "\n",
    "\n",
    "class RegexFinder(object):\n",
    "    def __init__(self, init_text, init_pattern=\"\", height=\"100px\", width=\"600px\"):\n",
    "           \n",
    "        # the areas for patterns and input strings\n",
    "        self.text_field = Textarea(init_text, layout=Layout(height=height, width=width))\n",
    "        self.pattern_field = Textarea(init_pattern, layout=Layout(height='30px'))\n",
    "        \n",
    "        # the boxes containing the fields and respective labels\n",
    "        self.text_box = Box([Label(value='Text Box'), self.text_field])\n",
    "        self.pattern_box = Box([Label(value='Pattern Box'), self.pattern_field])\n",
    "        \n",
    "        # a button to display results\n",
    "        self.match_button = Button(description='Match Pattern', )\n",
    "        self.match_button.on_click(self.match_pattern)\n",
    "        \n",
    "        # tell jupyter to display everything\n",
    "        display(self.text_box)\n",
    "        display(self.pattern_box)\n",
    "        display(self.match_button)\n",
    "        self.match_pattern(None)\n",
    "        \n",
    "    @property\n",
    "    def pattern(self):\n",
    "        # whenever we ask for the current pattern, create a re object\n",
    "        try:\n",
    "            return re.compile(self.pattern_field.value)\n",
    "        except:\n",
    "            raise ValueError(\"Bad Regex Pattern: {}, try again please!\".format(self.pattern_field.value))\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        # whenever we ask for the string input, grab the text_field's current value\n",
    "        return self.text_field.value    \n",
    "    \n",
    "    def match_pattern(self, b):\n",
    "        #clear any output\n",
    "        clear_output()\n",
    "        # create a formatted string, create a markdown object, and have Jupyter display it\n",
    "        if self.pattern.pattern != \"\":\n",
    "            \n",
    "            display(Markdown(self.format_match_markdown(self.text, self.pattern)))\n",
    "        else:\n",
    "            display(Markdown(self.text))\n",
    "            \n",
    "        \n",
    "    def format_match_markdown(self, text, pattern):\n",
    "        new_string = \"\"\n",
    "        last = 0\n",
    "        \n",
    "        # for each found match\n",
    "        for i in pattern.finditer(text):\n",
    "            # get the string index of the match\n",
    "            start, stop = i.span()\n",
    "            # append string_since_last_match + format(match) to result\n",
    "            new_string += text[last:start] + \"<b style='color:blue;'><u>\" + html.escape(text[start:stop]) +  \"</u></b>\"\n",
    "            # set string index since last match to end of current match\n",
    "            last = stop\n",
    "        \n",
    "        # grab anything leftover\n",
    "        new_string += text[last:]\n",
    "        return new_string\n",
    "        \n",
    "\n",
    "r = RegexFinder(\"Welcome to JupyterCon 2017!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather some data from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install wikipedia >> ~/wikipedia_install.log\n",
    "import wikipedia\n",
    "wiki = wikipedia.WikipediaPage('Timeline_of_Solar_System_exploration')\n",
    "intro_paragraph = \"\\n\".join(wiki.content.split('\\n')[11:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Exercise 1: Match the dates in this document\n",
    "\n",
    "(AA)\n",
    "\n",
    "e.g. [day] [month] [year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_datascience": {},
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "== 1950s ==\n",
       "\n",
       "1957\n",
       " Sputnik 1 – 4 October 1957 – First Earth orbiter\n",
       " Sputnik 2 – 3 November 1957 – Earth orbiter, first animal in orbit, a dog named Laika\n",
       "1958\n",
       " Explorer 1 – 1 February 1958 – Earth orbiter; first American orbiter, discovered Van Allen radiation belts\n",
       " Vanguard 1 – 17 March 1958 – Earth orbiter; oldest spacecraft still in Earth orbit\n",
       "1959\n",
       " Luna 1 – 2 January 1959 – First lunar flyby (attempted lunar impact?)\n",
       " Pioneer 4 – 3 March 1959 – Lunar flyby\n",
       " Luna 2 – 12 September 1959 – First lunar impact\n",
       " Luna 3 – 4 October 1959 – Lunar flyby; First images of far side of Moon\n",
       "\n",
       "\n",
       "== 1960s ==\n",
       "\n",
       "1960\n",
       " Pioneer 5 – 11 March 1960 – Interplanetary space investigations\n",
       "1961\n",
       " Venera 1 – 12 February 1961 – Venus flyby (contact lost before flyby)\n",
       " Vostok 1 – 12 April 1961 – First manned Earth orbiter\n",
       " Mercury-Redstone 3 – 5 May 1961 – First American in space\n",
       " Ranger 1 – 23 August 1961 – Attempted lunar test flight\n",
       " Ranger 2 – 18 November 1961 – Attempted lunar test flight\n",
       "1962\n",
       " Ranger 3 – 26 January 1962 – Attempted lunar impact (missed Moon)\n",
       " Mercury-Atlas 6 – 20 February 1962 – First American manned Earth orbiter\n",
       " Ranger 4 – 23 April 1962 – Lunar impact (but unintentionally became the first spacecraft to hit the lunar farside and returned no data)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finder = RegexFinder(intro_paragraph, height=\"200px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: \n",
    "Write a pattern that splits all the sentences in \"Separate Sentences\", and nothing in \"Single Sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Separate Sentences:\n",
       "-------------------\n",
       "assumes word senses. Within \n",
       "\n",
       "does the clustering. In the\n",
       "\n",
       "but when? It was hard to tell\n",
       "\n",
       "he arrive.\" After she had\n",
       "\n",
       "mess! He did not let it\n",
       "\n",
       "it wasn't hers!' She replied\n",
       "\n",
       "always thought so.) Then\n",
       "\n",
       "Single Sentences:\n",
       "-------------------\n",
       "in the U.S.A., people often\n",
       "\n",
       "John?\", he often thought, but\n",
       "\n",
       "weighed 17.5 grams\n",
       "\n",
       "well ... they'd better not\n",
       "\n",
       "A.I. has long been a very\n",
       "\n",
       "like that\", he thought\n",
       "\n",
       "but W. G. Grace never had much\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sents = \"\"\"\n",
    "Separate Sentences:\n",
    "-------------------\n",
    "assumes word senses. Within \n",
    "\n",
    "does the clustering. In the\n",
    "\n",
    "but when? It was hard to tell\n",
    "\n",
    "he arrive.\" After she had\n",
    "\n",
    "mess! He did not let it\n",
    "\n",
    "it wasn't hers!' She replied\n",
    "\n",
    "always thought so.) Then\n",
    "\n",
    "Single Sentences:\n",
    "-------------------\n",
    "in the U.S.A., people often\n",
    "\n",
    "John?\", he often thought, but\n",
    "\n",
    "weighed 17.5 grams\n",
    "\n",
    "well ... they'd better not\n",
    "\n",
    "A.I. has long been a very\n",
    "\n",
    "like that\", he thought\n",
    "\n",
    "but W. G. Grace never had much\n",
    "\n",
    "\"\"\"\n",
    "r = RegexFinder(sents, init_pattern=\"\", width=\"400px\", height=\"400px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecing it Together: segment sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_datascience": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I went to the store.', 'Then I ate garlic.', 'Then I ate some more.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def sent_segmenter(doc):\n",
    "    \"\"\"Returns a list of sentences from document\n",
    "    \n",
    "    doc: string\n",
    "    \n",
    "    returns: list\n",
    "    \"\"\"\n",
    "    \n",
    "    # our pattern for finding sentence barriers\n",
    "    pattern = re.compile('([a-z0-9])(?P<end_of_sentence>\\.|\\?|\\!)(?P<next_sentence>\\s|\\\"\\s|\\'\\s|\\)\\s)')\n",
    "    \n",
    "    def recurse_sent(text, sents):\n",
    "        match = pattern.search(text)\n",
    "        \n",
    "        # base case: if there is no sentence boundaries, add the current text\n",
    "        # to list of sentences, return sentences\n",
    "        if match is None:\n",
    "            sents.append(text)\n",
    "            return sents\n",
    "        \n",
    "        # other wise we have multiple sentences. add the current sentence to\n",
    "        # list of sentences, pass in remaining text to head of the function\n",
    "        else:\n",
    "            curr_sent_ends = match.end('end_of_sentence')\n",
    "            next_doc_begins = match.end('next_sentence')\n",
    "            sents.append(text[:curr_sent_ends])\n",
    "            return recurse_sent(text[next_doc_begins:], sents)\n",
    "    return recurse_sent(doc, [])\n",
    "\n",
    "text = \"I went to the store. Then I ate garlic. Then I ate some more.\"\n",
    "print(list(sent_segmenter(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', '', 'went', 'to', 'the', 'store.', 'Then', 'I', 'ate', 'garlic.', 'Then', 'I', 'ate', 'some', 'more.']\n"
     ]
    }
   ],
   "source": [
    "def word_tokenizer(doc):\n",
    "    \"\"\"Returns a list of sentences from document\n",
    "    \n",
    "    doc: string\n",
    "    \n",
    "    returns: list\n",
    "    \"\"\"    \n",
    "    pattern = re.compile('\\s')\n",
    "    return pattern.split(doc)\n",
    "    \n",
    "text = \"I  went to the store. Then I ate garlic. Then I ate some more.\"\n",
    "print(list(word_tokenizer(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### But there is a problem:\n",
    "\n",
    "**Tokenization**:\n",
    "* Inconsistencies with Rules:\n",
    "    * Given the pattern \"[word]. [next]\", is \".\" a token?\n",
    "    * \"I went to the store**.** Then I...\" $\\rightarrow$ yes, its a punctuation.\n",
    "    * \"Dr**.** White will now see you\" $\\rightarrow$ no, it's part of the token \"Dr.\"\n",
    "    * **SpaCy Solution**: define special exceptions.\n",
    "* Some characters, even if not separated by a space, should be their own tokens:\n",
    "    * The film made \\$192,200,000 at the box office **(US: \\$102,000,000).** $\\rightarrow$ [\"(\", \"\\$\",\"102,000,000\", \")\", \".\"]\n",
    "    * **SpaCy Solution**: tokenize prefixes and suffixes\n",
    "* Tokenization \"destroys\" the white space\n",
    "    * **SpaCy Solution**, tokenization defined by character indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Spacy's Tokenizer Works:\n",
    "* Split text by whitespace.\n",
    "* Iterate over space-separated substrings: \n",
    "    * The dog doesn't shop at Macy's (anymore).$\\rightarrow$ [The, dog, doesn't, shop, at, Macy's, (anymore).]\n",
    "* Check whether we have an explicitly defined rule for this substring. If we do, use it.\n",
    "    * doesn't $\\rightarrow$ [does, n't]\n",
    "* Otherwise, try to consume a prefix.\n",
    "* (anymore). $\\rightarrow$ [(, anymore).]\n",
    "* If we consumed a prefix, go back to the beginning of the loop, so that special-cases always get priority.\n",
    "* If we didn't consume a prefix, try to consume a suffix.\n",
    "    * anymore). $\\rightarrow$ [anymore, ), .]\n",
    "* If we can't consume a prefix or suffix, look for \"infixes\" — stuff like hyphens etc.\n",
    "* Once we can't consume any more of the string, handle it as a single token.\n",
    "    * [The, dog, does, n't, shop, at, Macy's, (, anymore, ), .]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Closer Look at Exceptions: how to tokenize \"don't\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: don't\n",
      "How to tokenize it: [{65: 'do', 73: 'do'}, {65: \"n't\", 75: 'RB', 73: 'not'}]\n",
      "Readble version: [{'LEMMA': 'do', 'ORTH': 'do'}, {'TAG': 'RB', 'ORTH': \"n't\", 'LEMMA': 'not'}]\n"
     ]
    }
   ],
   "source": [
    "### Tokenizer execptions:\n",
    "### What is Orth???\n",
    "from spacy.en import TOKENIZER_EXCEPTIONS\n",
    "from spacy.attrs import ORTH\n",
    "\n",
    "\n",
    "\n",
    "# how we should tokenize \"don't\"\n",
    "special_word = \"don't\"\n",
    "exception = TOKENIZER_EXCEPTIONS[\"don't\"]\n",
    "readable_exception = [{nlp.vocab.strings[j]:i[j] for j in i} for i in exception]\n",
    "print(\"Word: {}\".format(special_word))\n",
    "print(\"How to tokenize it: {}\".format(exception))\n",
    "print(\"Readble version: {}\".format(readable_exception))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A toy version of the SpaCy tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from spacy.attrs import ORTH\n",
    "import re\n",
    "\n",
    "if 'nlp' not in globals():\n",
    "    import spacy\n",
    "    nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    print(\"Now tokenizing:\\n {}\".format(doc))\n",
    "    print()\n",
    "    whitespace = re.compile(\"[\\s]+\")\n",
    "    \n",
    "    token_list = []\n",
    "    \n",
    "    # begin by iterating over candidate tokens, \n",
    "    # which are separated by white space\n",
    "    for token in whitespace.split(doc):\n",
    "        #expand the candidate tokens to the full list\n",
    "        new_tokens = iter_tokenize(token, [])\n",
    "        token_list.extend(new_tokens)\n",
    "    return token_list\n",
    "\n",
    "def indent(level):\n",
    "    return \"\\t\" * level\n",
    "\n",
    "def iter_tokenize(token, tokens, level=0):\n",
    "    if token in TOKENIZER_EXCEPTIONS:\n",
    "        for addition in TOKENIZER_EXCEPTIONS[token]:\n",
    "            print(indent(level), \"appending exception \", addition[ORTH])\n",
    "            tokens.append(addition[ORTH])\n",
    "        return tokens\n",
    "    else:\n",
    "        pre_match = nlp.tokenizer.prefix_search(token)\n",
    "        suff_match = nlp.tokenizer.suffix_search(token)     \n",
    "        if pre_match:\n",
    "            start, end = pre_match.start(), pre_match.end()\n",
    "            print(indent(level), \"Prefix case for {}\".format(token))\n",
    "            prefix, token = token[start:end], token[end:]\n",
    "            print(indent(level + 1), \"appending prefix \", prefix)\n",
    "            tokens.append(prefix)\n",
    "            tokens = iter_tokenize(token, tokens, level+1)\n",
    "        elif suff_match:\n",
    "            start, end = suff_match.start(), suff_match.end()\n",
    "            print(indent(level), \"Suffix case for {}\".format(token))            \n",
    "            token, suffix = token[:start], token[start:end]\n",
    "            tokens = iter_tokenize(token, tokens, level+1)\n",
    "            print(indent(level + 1), \"appending suffix \", suffix)            \n",
    "            tokens.append(suffix)\n",
    "        else:\n",
    "            print(indent(level), \"appending token \", token)            \n",
    "            tokens.append(token)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now tokenizing:\n",
      " She said 'don't forget to take out the trash!'\n",
      "\n",
      " appending token  She\n",
      " appending token  said\n",
      " Prefix case for 'don't\n",
      "\t appending prefix  '\n",
      "\t appending exception  do\n",
      "\t appending exception  n't\n",
      " appending token  forget\n",
      " appending token  to\n",
      " appending token  take\n",
      " appending token  out\n",
      " appending token  the\n",
      " Suffix case for trash!'\n",
      "\t Suffix case for trash!\n",
      "\t\t appending token  trash\n",
      "\t\t appending suffix  !\n",
      "\t appending suffix  '\n",
      "['She', 'said', \"'\", 'do', \"n't\", 'forget', 'to', 'take', 'out', 'the', 'trash', '!', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "example = \"She said 'don't forget to take out the trash!'\"\n",
    "tokens = tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Extending the spacy tokenizer\n",
    "\n",
    "* add a special case\n",
    "* modify the tokenizer's prefix/suffix/infix search patterns\n",
    "* create an entirely new tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "source": [
    "#### Adding a special case\n",
    "We might want to tokenize \"gimme\" as [\"gim\", \"me\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gimme\n",
      "that\n",
      "sandwich\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Default English model tokenization:\n",
    "example2 = u'Gimme that sandwich!'\n",
    "for token in nlp(example2):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gim\n",
      "me\n",
      "that\n",
      "sandwich\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# lets add a special case to our model's tokenizer\n",
    "from spacy.attrs import ORTH, LEMMA, TAG\n",
    "\n",
    "nlp.tokenizer.add_special_case('gimme', [{ORTH:u'gim', LEMMA:u'give', TAG:u\"VB\"}, {ORTH:'me'}])\n",
    "nlp.tokenizer.add_special_case('Gimme', [{ORTH:u'Gim', LEMMA:u'give', TAG:u\"VB\"}, {ORTH:'me'}])\n",
    "\n",
    "# tokenization with our special case added:\n",
    "example2 = u'Gimme that sandwich!'\n",
    "for token in nlp(example2):\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Examples:\n",
    "* willya -> will, you\n",
    "* tbt -> throw back thursdays\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise:\n",
    "* update the tokenizer with 3 additional special cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nlp.tokenizer.add_special_case('', [{ORTH:u'', LEMMA:u'', TAG:u\"\"}])\n",
    "# nlp.tokenizer.add_special_case('', [{ORTH:u'', LEMMA:u'', TAG:u\"\"}])\n",
    "# nlp.tokenizer.add_special_case('', [{ORTH:u'', LEMMA:u'', TAG:u\"\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the tokenizer\n",
    "We just modified the special exceptions component of the tokenizer. Sometimes we may need to modify our definitions of prefix, suffix.\n",
    "\n",
    "**Example**: keep hashtags and mentions as part of the original token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokenizer\n",
      "[Gim, me, that, sandwich, @John, #, food]\n",
      "New Tokenizer\n",
      "[Gimme, that, sandwich, @John, #food]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "exceptions = spacy.en.TOKENIZER_EXCEPTIONS\n",
    "prefixes = re.compile('''[\\[\\]\\(\\)\\'\\\"]''')\n",
    "suffixes = re.compile('''[\\[\\]\\(\\)\\'\\\"]''') \n",
    "\n",
    "custom_tokenizer = Tokenizer(nlp.vocab\n",
    "                             , exceptions\n",
    "                             , prefixes.search\n",
    "                             , suffixes.search\n",
    "                             , nlp.tokenizer.infix_finditer)\n",
    "\n",
    "example3 = \"Gimme that sandwich @John #food\"\n",
    "\n",
    "print(\"Original Tokenizer\")\n",
    "print(list(nlp.tokenizer(example3)))    \n",
    "print\n",
    "print(\"New Tokenizer\")\n",
    "print(list(custom_tokenizer(example3)))    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create an entirely new tokenizer\n",
    "Lastly, if you want to use an entirely different tokenization strategy. \n",
    "\n",
    "The Tokenizer must:\n",
    "* be initialized with a SpaCy language model object.\n",
    "* be callable, which returns a SpaCy Document object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Gimme, that, sandwich, @John, #food]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "class CustomTokenizer(object):\n",
    "    \n",
    "    def __init__(self, nlp):\n",
    "        self.vocab = nlp.vocab\n",
    "        self.whitespace = re.compile(\"\\s\")\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = self.tokenize(text)\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return self.whitespace.split(text)\n",
    "    \n",
    "\n",
    "nlp.make_doc = CustomTokenizer(nlp)\n",
    "list(nlp(example3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding your Tokenizer to the language pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#method 1\n",
    "nlp = spacy.load('en')\n",
    "nlp.make_doc = custom_tokenizer\n",
    "\n",
    "#method 2\n",
    "nlp = spacy.load('en', make_doc=custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_datascience": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
