{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging: Agenda\n",
    "* <a href=\"#section1\">What are parts of speech? Why are they useful?</a>\n",
    "* <a href=\"#section2\">How do you use them with SpaCy?</a>\n",
    "* <a href=\"#section3\">How do we infer them?</a>\n",
    "* <a href=\"#section4\"> How do we learn them with SpaCy?</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "util/installers.ipynb:23: UserWarning: Downloading Spacy Model\n",
      "  \"        import gensim\\n\",\n",
      "util/installers.ipynb:29: UserWarning: Installing nltk\n",
      "  \"    nlp = spacy.load('en')\\n\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata .............\n",
      "Solving package specifications: .\n",
      "\n",
      "Package plan for installation in environment /opt/conda:\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    nltk: 3.2.4-py36_0 conda-forge\n",
      "\n",
      "nltk-3.2.4-py3 100% |################################| Time: 0:00:00   2.40 MB/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "util/installers.ipynb:39: UserWarning: Installing keras\n",
      "  \"    try:\\n\",\n",
      "util/installers.ipynb:45: UserWarning: Installing skater\n",
      "  \"        import keras\\n\",\n",
      "util/installers.ipynb:48: UserWarning: You may need to refresh the notebook.\n",
      "  \"        !conda install keras -y >> ~/install.log\\n\",\n"
     ]
    }
   ],
   "source": [
    "%run util/installers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "from spacy.en import English\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from spacy.tokens import Doc\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "\n",
    "def as_list(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return list(f(*args, **kwargs))\n",
    "    \n",
    "    return wrapper\n",
    "    \n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "map_ = as_list(map)\n",
    "filter_ = as_list(filter)\n",
    "zip_ = as_list(zip)\n",
    "\n",
    "def rep_sentences(texts):\n",
    "    html = []\n",
    "    for text in texts:\n",
    "        html.append(rep_sentence(text))\n",
    "    return HTML(\"\".join(html))\n",
    "\n",
    "def rep_sentence(text, display_pos = True, words_to_highlight=[]):\n",
    "    html_colors = ['SkyBlue'\n",
    "               ,'red'\n",
    "               ,'YellowGreen'\n",
    "               ,'yellow'\n",
    "               ,'orange'\n",
    "               ,'pink'\n",
    "               ,'brown'\n",
    "               ,'purple'\n",
    "               , 'CadetBlue'\n",
    "                ,'DarkKhaki'\n",
    "                ,'DarkSalmon'\n",
    "                ,'Gold'    \n",
    "              ]\n",
    "    doc = nlp(text)\n",
    "    n_words = len(doc)\n",
    "    unique_pos = list(set(map(lambda x: x.pos_, doc)))\n",
    "    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n",
    "    css = [\"<style>.word{font-weight:bold;}</style>\"]\n",
    "    for pos in unique_pos:\n",
    "        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n",
    "    css = \"\".join(css)\n",
    "\n",
    "    html = [\"<table width=100%>\"]\n",
    "    html.append(css)\n",
    "    html.append(\"<tr>\")            \n",
    "    for i in range(n_words):\n",
    "        word_string= doc[i].orth_\n",
    "        if word_string in words_to_highlight:\n",
    "            word_string = \"<u>{}</u>\".format(word_string)\n",
    "        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n",
    "    html.append(\"</tr>\")\n",
    "    if display_pos:\n",
    "        html.append(\"<tr>\")            \n",
    "        for i in range(n_words):\n",
    "            pos = doc[i].pos_\n",
    "            color = pos_to_color[pos]\n",
    "            html.append(\"<td><span class='{0}'>{0}</span></td>\".format(pos))\n",
    "        html.append(\"</tr>\")\n",
    "    html = \"\".join(html)\n",
    "    return html\n",
    "\n",
    "\n",
    "\n",
    "def custom_tag_table(list_of_word_tag_tuples):\n",
    "    html_colors = ['SkyBlue'\n",
    "               ,'red'\n",
    "               ,'YellowGreen'\n",
    "               ,'yellow'\n",
    "               ,'orange'\n",
    "               ,'pink'\n",
    "               ,'brown'\n",
    "               ,'MediumPurple'\n",
    "               , 'CadetBlue'\n",
    "                ,'DarkKhaki'\n",
    "                ,'DarkSalmon'\n",
    "                ,'Gold'    \n",
    "              ]\n",
    "    \n",
    "    n_words = len(list_of_word_tag_tuples)\n",
    "    words, pos_list = zip(*list_of_word_tag_tuples)\n",
    "    unique_pos = list(set([pos for pair in pos_list for pos in pair]))\n",
    "    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n",
    "    css = [\"<style>.word{font-weight:bold;}</style>\"]\n",
    "    for pos in unique_pos:\n",
    "        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n",
    "    css = \"\".join(css)\n",
    "\n",
    "    html = [\"<table width=100%>\"]\n",
    "    html.append(css)\n",
    "    for i in range(n_words):\n",
    "        html.append(\"<tr>\")            \n",
    "        word_string= words[i]\n",
    "        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n",
    "        row = []\n",
    "        pos_sublist = pos_list[i]\n",
    "        for pos in pos_sublist:\n",
    "            entry = \"<span class='{0}'>{0}</span> \".format(pos)\n",
    "            #print entry\n",
    "            row.append(entry)\n",
    "        row = \"\".join(row)\n",
    "        html.append(\"<td>{}</td>\".format(row))\n",
    "        html.append(\"</tr>\")\n",
    "    return \"\".join(html)\n",
    "        \n",
    "    \n",
    "\n",
    "def nltk_corpus(corpus_name):\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    return corpus\n",
    "\n",
    "#read nltk corpora\n",
    "def nltk_reader(corpus_name, limit = None):\n",
    "    corpus = nltk_corpus(corpus_name)\n",
    "    fileids = corpus.fileids()\n",
    "    \n",
    "    if limit:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids[:limit])\n",
    "    else:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids)\n",
    "    return doc_iter\n",
    "\n",
    "universal_tags = [\n",
    "     ['Open Class Words','ADJ','adjective', 'happy, great, technical']\n",
    "    ,['Open Class Words','ADV','adverb', 'happily, greatly, technically']\n",
    "    ,['Open Class Words','INTJ','interjection', 'ouch, wow']\n",
    "    ,['Open Class Words','NOUN','noun', 'happiness, greatness, technicality']\n",
    "    ,['Open Class Words','PROPN','proper noun', 'Jupyter, Lakers']\n",
    "    ,['Open Class Words','VERB','verb', 'run, enlighten']\n",
    "    ,['Closed Class Words','ADP','adposition', 'in, under, towards']\n",
    "    ,['Closed Class Words','AUX','auxiliary', 'should [eat], had [been]']\n",
    "    ,['Closed Class Words','CCONJ','coordination conjunction', 'and, or, but']\n",
    "    ,['Closed Class Words','DET','determiner', 'the, a']\n",
    "    ,['Closed Class Words','NUM','numeral', 'five, 5']\n",
    "    ,['Closed Class Words','PART','particle', \"'s, not\"]\n",
    "    ,['Closed Class Words','PRON','pronoun', 'him, she']\n",
    "    ,['Closed Class Words','SCONJ','subordinating conjection', 'that, if']\n",
    "    ,['Other','PUNCT','punctuation', \"., ,\"]\n",
    "    ,['Other','SYM','symbol', \"#, $\"]\n",
    "    ,['Other','X','other', \"#2017, @JupyterCon\"]\n",
    "]\n",
    "tag_table = pd.DataFrame(universal_tags, columns = ['Category','Abbrev','Part of Speech', 'Example'])\n",
    "tag_table = tag_table.set_index(['Category','Abbrev'])\n",
    "\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section1\"></a>\n",
    "\n",
    "### What are Parts of Speech?\n",
    "\n",
    "<a name=\"universaltags\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Part of Speech</th>\n",
       "      <th>Example</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">Open Class Words</th>\n",
       "      <th>ADJ</th>\n",
       "      <td>adjective</td>\n",
       "      <td>happy, great, technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>adverb</td>\n",
       "      <td>happily, greatly, technically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>interjection</td>\n",
       "      <td>ouch, wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>noun</td>\n",
       "      <td>happiness, greatness, technicality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>proper noun</td>\n",
       "      <td>Jupyter, Lakers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>verb</td>\n",
       "      <td>run, enlighten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">Closed Class Words</th>\n",
       "      <th>ADP</th>\n",
       "      <td>adposition</td>\n",
       "      <td>in, under, towards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>auxiliary</td>\n",
       "      <td>should [eat], had [been]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>coordination conjunction</td>\n",
       "      <td>and, or, but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>determiner</td>\n",
       "      <td>the, a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>numeral</td>\n",
       "      <td>five, 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>particle</td>\n",
       "      <td>'s, not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>pronoun</td>\n",
       "      <td>him, she</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>subordinating conjection</td>\n",
       "      <td>that, if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Other</th>\n",
       "      <th>PUNCT</th>\n",
       "      <td>punctuation</td>\n",
       "      <td>., ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>symbol</td>\n",
       "      <td>#, $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>other</td>\n",
       "      <td>#2017, @JupyterCon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Part of Speech  \\\n",
       "Category           Abbrev                             \n",
       "Open Class Words   ADJ                    adjective   \n",
       "                   ADV                       adverb   \n",
       "                   INTJ                interjection   \n",
       "                   NOUN                        noun   \n",
       "                   PROPN                proper noun   \n",
       "                   VERB                        verb   \n",
       "Closed Class Words ADP                   adposition   \n",
       "                   AUX                    auxiliary   \n",
       "                   CCONJ   coordination conjunction   \n",
       "                   DET                   determiner   \n",
       "                   NUM                      numeral   \n",
       "                   PART                    particle   \n",
       "                   PRON                     pronoun   \n",
       "                   SCONJ   subordinating conjection   \n",
       "Other              PUNCT                punctuation   \n",
       "                   SYM                       symbol   \n",
       "                   X                          other   \n",
       "\n",
       "                                                      Example  \n",
       "Category           Abbrev                                      \n",
       "Open Class Words   ADJ                happy, great, technical  \n",
       "                   ADV          happily, greatly, technically  \n",
       "                   INTJ                             ouch, wow  \n",
       "                   NOUN    happiness, greatness, technicality  \n",
       "                   PROPN                      Jupyter, Lakers  \n",
       "                   VERB                        run, enlighten  \n",
       "Closed Class Words ADP                     in, under, towards  \n",
       "                   AUX               should [eat], had [been]  \n",
       "                   CCONJ                         and, or, but  \n",
       "                   DET                                 the, a  \n",
       "                   NUM                                five, 5  \n",
       "                   PART                               's, not  \n",
       "                   PRON                              him, she  \n",
       "                   SCONJ                             that, if  \n",
       "Other              PUNCT                                 ., ,  \n",
       "                   SYM                                   #, $  \n",
       "                   X                       #2017, @JupyterCon  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='applications'></a>\n",
    "### Applications\n",
    "* Rule based systems:\n",
    "    * information retrieval\n",
    "    * <a href=\"#qacode\">Example of rule based question answering component</a>\n",
    "    \n",
    "* Feature engineering for statistical models\n",
    "    * Useful for language models (which in turn can be helpful for translation, etc...)\n",
    "    * <a href=\"#wordsense\">Feature for word disambiguation</a>\n",
    "    * They often define syntactic requirements, so are helpful for parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.DET{background-color:SkyBlue;}</style><style>.NOUN{background-color:red;}</style><style>.PUNCT{background-color:YellowGreen;}</style><style>.VERB{background-color:yellow;}</style><style>.ADP{background-color:orange;}</style><style>.PRON{background-color:pink;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>get</span></td><td><span class='word'>a</span></td><td><span class='word'>discount</span></td><td><span class='word'>on</span></td><td><span class='word'>newspapers</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr><table width=100%><style>.word{font-weight:bold;}</style><style>.NOUN{background-color:SkyBlue;}</style><style>.PUNCT{background-color:red;}</style><style>.VERB{background-color:YellowGreen;}</style><style>.ADP{background-color:yellow;}</style><style>.PRON{background-color:orange;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>discount</span></td><td><span class='word'>that</span></td><td><span class='word'>newspaper</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = 'I get a discount on newspapers.'\n",
    "sentence2 = 'I discount that newspaper.'\n",
    "\n",
    "rep_sentences([sentence1, sentence2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section2\"></a>\n",
    "### Parts of Speech with SpaCy\n",
    "Lets see how we can access parts of speech with spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos (coarse)</th>\n",
       "      <th>pos (fine)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discount</th>\n",
       "      <td>discount</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>get</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newspapers</th>\n",
       "      <td>newspaper</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>on</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                lemma pos (coarse) pos (fine)\n",
       "I              -PRON-         PRON        PRP\n",
       "a                   a          DET         DT\n",
       "discount     discount         NOUN         NN\n",
       "get               get         VERB        VBP\n",
       "newspapers  newspaper         NOUN        NNS\n",
       "on                 on          ADP         IN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Accessing\n",
    "doc = nlp('I get a discount on newspapers')\n",
    "\n",
    "tags = {}\n",
    "\n",
    "for word in doc:\n",
    "    tags[word.orth_] = {'lemma': word.lemma_, 'pos (coarse)': word.pos_, 'pos (fine)':word.tag_}\n",
    "    \n",
    "pd.DataFrame(tags).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000338590165"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without POS tags, our word vectors arent disambiguating these...\n",
    "discount1 = nlp('I get a discount on newspapers.')[3]\n",
    "discount2 = nlp('I discount that newspaper.')[1]\n",
    "discount1.similarity(discount2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: Building word vectors that are Part of Speech specific\n",
    "Steps:\n",
    "* get documents\n",
    "* tokenize the documents, and append the part of speech to each token, e.g. dog||||NOUN\n",
    "* train a word2vec model with gensim\n",
    "* compare the most similar words of 'back||||VERB' vs 'back||||NOUN' (or other combo)\n",
    "\n",
    "* Hints:\n",
    "    * model.wv.vocab contains the vocabulary.\n",
    "    * using a completely unique join character will make it easier to split later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def return_documents():\n",
    "    \"\"\"\n",
    "    Returns a list of documents, where each document is a string\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    dataset = fetch_20newsgroups()\n",
    "    corpus = dataset.data\n",
    "    return corpus\n",
    "\n",
    "def tokenize_and_tag_documents(documents, nlp, sep_char=\"||||\"):\n",
    "    \"\"\"\n",
    "    Returns a list of lists of tokens. Each token has been \n",
    "    concatenated with its part of speech.\n",
    "    \n",
    "    Hint 1: nlp.pipe(documents) is faster than nlp(doc) for doc in documents\n",
    "    Hint 2: Part of speech accessible via token.pos_\n",
    "    Hint 3: iterating over a document yields tokens.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def build_model(tokenized_docs):\n",
    "    \"\"\"\n",
    "    Returns a gensim Word2Vec model trained on our corpus.\n",
    "    Hint 1: use gensim.models.Word2Vec\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "documents = return_documents()\n",
    "tokenized_and_tagged_documents = tokenize_and_tag_documents(documents, nlp)\n",
    "model = build_model(tokenized_and_tagged_documents)\n",
    "\n",
    "if model:\n",
    "    print(model.most_similar('real||||ADJ'))\n",
    "    print()\n",
    "    print(model.most_similar('real||||ADV'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/datascienceinc/jupytercon-2017/blob/master/tutorial/Solutions.ipynb#POS-1\" id=\"solution1\">Solution</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "\n",
    "var url = window.location.href\n",
    "var link = document.getElementById('solution1')\n",
    "link.innerHTML = \"Bye\"\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section3\"></a>\n",
    "### How do we infer parts of speech?\n",
    "Take a moment to determine the parts of speech of the underlined words.\n",
    "* e.g.: VERB, NOUN, PRON, PROPN, ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.PUNCT{background-color:SkyBlue;}</style><style>.VERB{background-color:red;}</style><style>.PART{background-color:YellowGreen;}</style><style>.PRON{background-color:yellow;}</style><style>.NOUN{background-color:orange;}</style><style>.ADP{background-color:pink;}</style><style>.ADJ{background-color:brown;}</style><style>.PROPN{background-color:purple;}</style><style>.DET{background-color:CadetBlue;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>was</span></td><td><span class='word'><u>loble</u></span></td><td><span class='word'>to</span></td><td><span class='word'>find</span></td><td><span class='word'>the</span></td><td><span class='word'><u>effix</u></span></td><td><span class='word'>by</span></td><td><span class='word'><u>klepping</u></span></td><td><span class='word'>the</span></td><td><span class='word'><u>Dongle</u></span></td><td><span class='word'>search</span></td><td><span class='word'>engine</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='ADJ'>ADJ</span></td><td><span class='PART'>PART</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='ADP'>ADP</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='PROPN'>PROPN</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import Button\n",
    "\n",
    "class reveal(object):\n",
    "    def __init__(self):\n",
    "        self.text = 'I was loble to find the effix by klepping the Dongle search engine.'\n",
    "        self.toggle = Button(description='Toggle POS', )\n",
    "        self.toggle.on_click(self.toggle_pos)\n",
    "        \n",
    "        self.state = False\n",
    "        display(self.toggle)\n",
    "        self.display()\n",
    "        \n",
    "    def toggle_pos(self, b):\n",
    "        self.state = not self.state\n",
    "        self.display()\n",
    "        \n",
    "    def display(self):\n",
    "        clear_output()\n",
    "        #display(self.toggle)\n",
    "        display(\n",
    "            HTML(\n",
    "                rep_sentence(\n",
    "                    self.text, \n",
    "                    display_pos = self.state, \n",
    "                    words_to_highlight=['loble','effix', 'klepping','Dongle'])\n",
    "            )\n",
    "        )\n",
    "        \n",
    "# !jupyter nbextension enable --py widgetsnbextension --sys-prefix        \n",
    "r = reveal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinants of Part of Speech:\n",
    "\n",
    "| Feature | Notes | Example|\n",
    "|------|------|------|\n",
    "|   Word Identity  | Some words can only be used in a single way; we can memorize these.| \"the\" -> determiner| \n",
    "| Word Shape|Capitalization, dashes,  |\"I stayed at the Park Hotel.\"|\n",
    "|Neighboring parts of speech|There are common patterns what tags can neighbor others|\"to the beach\" (noun following determiner)|\n",
    "|Morphological Structures|Word prefixes and suffixes can rule out certain tag types|\"-ly\" -> adverb|\n",
    "|Syntactic Dependencies|Syntax may establish expectations that only certain tags can logically fill|\"I was told __\" -> adpositional phrase or object entity|\n",
    "|?|?|?|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section4\"></a>\n",
    "### Homework: Training your own tagger with SpaCy\n",
    "\n",
    "**Steps** : \n",
    "* load <a href=\"#load_data\">**training data**</a>, where each observation is represented as (list_of_words, list_of_tags)\n",
    "    * [('Darkness', 'fell', '.'), ('NN', 'VBD', '.')]\n",
    "\n",
    "\n",
    "* Pick a <a href=\"#model_dir\">**model directory**</a>. Using the existing English model will allow us to leverage lexeme information, including Brown clusters, which is an excellent feature for tagging.\n",
    "    * nlp = spacy.load('en')\n",
    "    * nlp.save_to_directory(modelpath)\n",
    "    \n",
    "    \n",
    "* Look at your dataset. Build a <a href=\"#tagmap\">**tag map**</a> mapping from the part of speech tags to the <a href=\"#universaltagset\">universal tagset</a>.\n",
    "\n",
    "    * tagmap = {'ADJ': {POS: 'ADJ'},'HASHTAG': {POS: \"NOUN\"}, ...}\n",
    "    \n",
    "\n",
    "* Decide which <a href=\"#featureextractors\">**features**</a> to use\n",
    "    * features = [ (W_shape,), (P1_pos,),(P1_suffix),(P2_pos,), (N1_pos,),(N1_suffix),(N2_pos,) ]\n",
    "    * These ^^ are atomic predictors.\n",
    "    * Atomic predictors can be combined, but there is no API to add new ones.\n",
    "* create a <a href=\"#vocab\">**vocabulary object, a statistical model, and a tagger**</a>\n",
    "\n",
    "    * vocab = Vocab.load(modelpath, feature_extractors, lemmatizer, tagmap)\n",
    "    * statistical_model = spacy.tagger.TaggerModel(features)\n",
    "    * tagger = spacy.tagger.Tagger(vocab, statistical_model)\n",
    "\n",
    "\n",
    "* <a href=\"#training\">**Train the model**</a>\n",
    "* <a href=\"#save\">**Save the model**</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"load_data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n"
     ]
    }
   ],
   "source": [
    "#load conll2000 corpus\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "def nltk_corpus(corpus_name):\n",
    "    '''returns nltk corpus by name. if not loaded, download.'''\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    if x == '-LCB-':\n",
    "        return '{'\n",
    "    elif x=='-RCB-':\n",
    "        return '}'\n",
    "    elif x == '-RRB-':\n",
    "        return \")\"\n",
    "    elif x == '-LRB-':\n",
    "        return \"(\"\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def conll_to_data():\n",
    "    corpus = nltk_corpus('conll2000')\n",
    "    all_data= map_(lambda x: [(clean(i[0]), i[1]) for i in x], corpus.iob_sents())\n",
    "    all_data = [zip_(*i) for i in all_data]\n",
    "    return all_data\n",
    "\n",
    "c2000 = conll_to_data()\n",
    "training_data, testing_data = train_test_split(c2000, test_size = .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#section4\">Back</a>\n",
    "<a name=\"model_dir\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import PosixPath\n",
    "import os\n",
    "home = os.path.expanduser('~')\n",
    "modelpath = PosixPath('{}/mymodel'.format(home))\n",
    "if not modelpath.exists():\n",
    "    modelpath.mkdir()\n",
    "    \n",
    "nlp.save_to_directory(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#section4\">Back</a>\n",
    "<a href=\"tagmap\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.language_data import TAG_MAP\n",
    "from spacy.attrs import POS\n",
    "from spacy.symbols import PUNCT\n",
    "\n",
    "def adjust_tagmap(tagmap):\n",
    "    tagmap['('] = tagmap['-LRB-']\n",
    "    tagmap[')'] = tagmap['-RRB-']\n",
    "    tagmap['{'] = tagmap['-LRB-']\n",
    "    tagmap['}'] = tagmap['-RRB-']\n",
    "    tagmap['$'] = {POS: PUNCT}\n",
    "    return tagmap\n",
    "\n",
    "tagmap = adjust_tagmap(TAG_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#section4\">Back</a>\n",
    "<a name=\"featureextractors\"></a>\n",
    "### Features\n",
    "\n",
    "* Example features: pos of previous word, identity of current word, etc...\n",
    "* spacy.tagger.N1_cluster, spacy.tagger.N1_pos, etc...\n",
    "* which word?\n",
    "    * N1: Next\n",
    "    * N0: Current\n",
    "    * P1: Previous\n",
    "    * etc...\n",
    "* which attribute:\n",
    "    * prefix\n",
    "    * tag\n",
    "    * cluster\n",
    "    * etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.tagger import *\n",
    "\n",
    "\n",
    "features = [\n",
    "    #current word attributes\n",
    "    (W_orth,),(W_shape,),(W_cluster,),(W_flags,),(W_suffix,),(W_prefix,),\n",
    "\n",
    "    #-1 word attributes    \n",
    "    (P1_pos,),(P1_cluster,),(P1_flags,),(P1_suffix,),\n",
    "\n",
    "    #-2 word attributes     \n",
    "    (P2_pos,),(P2_cluster,),(P2_flags,),\n",
    "\n",
    "    #+1 word attributes    \n",
    "    (N1_orth,),(N1_suffix,),(N1_cluster,),(N1_flags,),    \n",
    "\n",
    "    #+2 word attributes    \n",
    "    (N2_orth,),(N2_cluster,),(N2_flags,),\n",
    "\n",
    "    #combination attributes\n",
    "    (P1_lemma, P1_pos),(P2_lemma, P2_pos), (P1_pos, P2_pos),(P1_pos, W_orth)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#section4\">Back</a>\n",
    "<a name=\"vocab\"></a>\n",
    "### Vocabulary, Tagger, and Statistical Model\n",
    "* The **Vocab** object will receive all the lexeme data (Brown clusters, word vectors, etc) from the English model.\n",
    "* The **Statistical Model**  will consume the features we defined, using them to make predictions.\n",
    "* The **Tagger** will consume our vocabulary object, and our statistical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: <function spacy.orth.is_alpha>,\n",
       " 2: <function spacy.orth.is_ascii>,\n",
       " 3: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 4: <function spacy.orth.is_lower>,\n",
       " 5: <function spacy.orth.is_punct>,\n",
       " 6: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 7: <function spacy.orth.is_title>,\n",
       " 8: <function spacy.orth.is_upper>,\n",
       " 9: <function spacy.orth.like_url>,\n",
       " 10: <function spacy.orth.like_number>,\n",
       " 11: <function spacy.orth.like_email>,\n",
       " 12: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 13: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 14: <function spacy.orth.is_bracket>,\n",
       " 15: <function spacy.orth.is_quote>,\n",
       " 16: <function spacy.orth.is_left_punct>,\n",
       " 17: <function spacy.orth.is_right_punct>,\n",
       " 66: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 67: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 68: <function spacy.orth.word_shape>,\n",
       " 69: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 70: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 72: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 81: <cyfunction load.<locals>.<lambda> at 0x7f8523c4dd38>,\n",
       " 82: <function spacy.en.English.Defaults.<lambda>>}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.Defaults.lex_attr_getters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "\n",
    "def get_lemmatizer():\n",
    "    return None\n",
    "\n",
    "def make_tagger(vocab, templates):\n",
    "    model = spacy.tagger.TaggerModel(templates)\n",
    "    return spacy.tagger.Tagger(vocab,model)\n",
    "\n",
    "def get_feature_extractors(nlp):\n",
    "    #return nlp.vocab.lex_attr_getters\n",
    "    return nlp.Defaults.lex_attr_getters\n",
    "\n",
    "#get requirements\n",
    "\n",
    "feature_extractors = get_feature_extractors(nlp)\n",
    "lemmatizer = get_lemmatizer()\n",
    "vocab = Vocab.load(modelpath, feature_extractors, lemmatizer, tagmap)\n",
    "statistical_model = spacy.tagger.TaggerModel(features)\n",
    "tagger = spacy.tagger.Tagger(vocab, statistical_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PUNCT'),\n",
       " ('can', 'PUNCT'),\n",
       " ('always', 'PUNCT'),\n",
       " ('learn', 'PUNCT'),\n",
       " ('more', 'PUNCT'),\n",
       " ('by', 'PUNCT'),\n",
       " ('reading', 'PUNCT')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The untrained tagger is awful...\n",
    "words = ['You','can','always','learn','more','by','reading']\n",
    "doc = Doc(vocab, words = words)\n",
    "tagger(doc)\n",
    "map_(lambda x: (x.orth_, x.pos_), doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Training Accuracy: 0.0 Pre Test Accuracy 0.0\n",
      "Iteration 0 Training Accuracy: 95.78076064556761 Test Accuracy 94.80484103202708\n",
      "Iteration 1 Training Accuracy: 97.5167839867699 Test Accuracy 96.08903763085185\n",
      "Iteration 2 Training Accuracy: 98.71040714288775 Test Accuracy 97.19422500681013\n",
      "Iteration 3 Training Accuracy: 98.69883936642859 Test Accuracy 97.09693738568704\n",
      "Iteration 4 Training Accuracy: 99.11527931895787 Test Accuracy 97.32653617153754\n",
      "Iteration 5 Training Accuracy: 99.43232208117152 Test Accuracy 97.5522434525431\n",
      "Iteration 6 Training Accuracy: 99.01373994781648 Test Accuracy 97.15141845351597\n",
      "Iteration 7 Training Accuracy: 99.50087186759609 Test Accuracy 97.47052185079971\n",
      "Iteration 8 Training Accuracy: 99.42289648553813 Test Accuracy 97.40825777328092\n",
      "Iteration 9 Training Accuracy: 99.7952075130566 Test Accuracy 97.8674553449819\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "\n",
    "def predict_cycle(vocab, tagger, data, train=True):\n",
    "    \"\"\"For each document in data, creates a document and tags\n",
    "    it. Creates a goldparse object to hold the ground truth label.\n",
    "    If train=True, the tagger's statistical model is updated with the \n",
    "    result.\"\"\"\n",
    "    \n",
    "    scorer = Scorer()\n",
    "    \n",
    "    for words, tags in data:\n",
    "        #create a document, passing in words to become tokens\n",
    "        doc = Doc(vocab, words = words)\n",
    "        tagger(doc)\n",
    "        gold = GoldParse(doc, tags=tags)   \n",
    "\n",
    "        scorer.score(doc, gold)              \n",
    "        \n",
    "        if train:\n",
    "            #train the model        \n",
    "            tagger.update(doc, gold)\n",
    "    return tagger, scorer\n",
    "\n",
    "def train(vocab, tagger, training_data, testing_data, epochs = 10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tagger (spacy.tagger.Tagger): \n",
    "        The tagger to train.\n",
    "        \n",
    "    training_data (list):\n",
    "        Training data containing words and annotated tags. \n",
    "        Should have form: [(word1, word2,...),(tag1, tag2, .....)]\n",
    "        \n",
    "    epochs (int):\n",
    "        number of training iterations\n",
    "        \n",
    "    verbose (Bool):\n",
    "        whether to track and print training accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    tagger, pre_train_scorer = predict_cycle(vocab, tagger, training_data, train=False)\n",
    "    tagger, pre_test_scorer = predict_cycle(vocab, tagger, testing_data, train=False)\n",
    "    \n",
    "    print(\"Pre Training Accuracy: {0} Pre Test Accuracy {1}\".format(pre_train_scorer.tags_acc, \n",
    "                                                                    pre_test_scorer.tags_acc))\n",
    "    \n",
    "    for train_cycle in range(epochs):\n",
    "            \n",
    "        tagger, _ = predict_cycle(vocab, tagger, training_data, train=True)\n",
    "        \n",
    "        \n",
    "        tagger, train_scorer = predict_cycle(vocab, tagger, training_data, train=False)\n",
    "        tagger, test_scorer = predict_cycle(vocab, tagger, testing_data, train=False)\n",
    "\n",
    "        print(\"Iteration {0} Training Accuracy: {1} Test Accuracy {2}\".format(train_cycle, \n",
    "                                                                              train_scorer.tags_acc, \n",
    "                                                                              test_scorer.tags_acc))\n",
    "        #shuffle data    \n",
    "        np.random.shuffle(training_data)\n",
    "    \n",
    "    tagger.model.end_training()\n",
    "        \n",
    "    return tagger\n",
    "\n",
    "tagger = train(vocab, tagger, training_data, testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out the new tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRON'),\n",
       " ('can', 'VERB'),\n",
       " ('always', 'ADV'),\n",
       " ('learn', 'VERB'),\n",
       " ('more', 'ADJ'),\n",
       " ('by', 'ADP'),\n",
       " ('reading', 'VERB')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The untrained tagger is awful...\n",
    "words = ['You','can','always','learn','more','by','reading']\n",
    "doc = Doc(vocab, words = words)\n",
    "tagger(doc)\n",
    "map_(lambda x: (x.orth_, x.pos_), doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"save\"></a>\n",
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_tagger(tagger, model_dir):\n",
    "    if model_dir is not None:\n",
    "        tagger.model.dump(str(model_dir / 'pos' / 'model'))\n",
    "        with (model_dir / 'vocab' / 'strings.json').open('w') as file_:\n",
    "            tagger.vocab.strings.dump(file_)\n",
    "            \n",
    "save_tagger(tagger, modelpath)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#section4\">Back</a>\n",
    "<a name=\"training\"></a>\n",
    "### More on Spacy's Statistical Model: Perceptron\n",
    "\n",
    "##### Neuron Prediction:\n",
    "\n",
    "**Inputs:** \n",
    "\n",
    "$<x_1, x_2, ..., x_n>$\n",
    "\n",
    "**Each Neuron j:**\n",
    "\n",
    "$prediction_j  = \\bigg[\\sum_{d=1}^D w_dx_{dj} \\bigg] + b > 0$\n",
    "\n",
    "##### Perceptron Learning:\n",
    "```\n",
    "For each training epoch:\n",
    "    For each feature_vector, label:\n",
    "        prediction = weights * feature_vector + bias\n",
    "\n",
    "        if sign(label) != sign(prediction):\n",
    "            weights = weights + (label*feature_vector)\n",
    "            bias = bias + label\n",
    "shuffle data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PerceptronClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.iter = 0\n",
    "\n",
    "    def fit(self, X, y, epochs=100):\n",
    "        \"\"\"Fits self.weights, self.biases \"\"\"\n",
    "        self.initialize(X)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            for row, label in zip(X, y):\n",
    "                prediction = self.predict(row)\n",
    "                self.update(prediction, label, row)\n",
    "                self.iter += 1\n",
    "            X, y = shuffle(X, y) #important to reshuffle to avoid getting trapped\n",
    "            \n",
    "    def initialize(self, X):\n",
    "        self.bias = 0\n",
    "        self.weights = np.zeros(X.shape[1])#[:, np.newaxis]\n",
    "                    \n",
    "    def update(self, prediction, label, row):\n",
    "        \"\"\"Updates weights and biases based on the ground truth label\n",
    "        and the row\"\"\"\n",
    "        \n",
    "        if prediction*label <= 0:\n",
    "            update = label *  row            \n",
    "            self.weights += update\n",
    "            self.bias += label\n",
    "        \n",
    "    def predict_score(self, x):\n",
    "        \"\"\"Generates scores of \"x\". Uses self.weights and self.bias\"\"\"\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\" Converts prediction scores to 1s ands 0s.\"\"\"\n",
    "        predictions = np.where(self.predict_score(x) > 0, 1, -1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/datascienceinc/jupytercon-2017/blob/master/tutorial/Solutions.ipynb#POS-2\">solution</a>\n",
    "\n",
    "Lets see how it does on some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Rate: 0.655\n"
     ]
    }
   ],
   "source": [
    "#generate some data\n",
    "def generate_data(dim=1, N=1000):\n",
    "    dim = 1\n",
    "    true_b = np.random.normal(0, 2)\n",
    "    X = np.random.normal(0, 10, size=(1000, dim))\n",
    "    true_w = np.random.normal(0, 2, size=(dim,))\n",
    "    err = np.random.normal(0, 10, size = N)\n",
    "    labels = (np.dot(X, true_w) + err > true_b).astype(float)\n",
    "    labels[np.where(labels==0)] = -1\n",
    "    return X, labels, true_w, true_b\n",
    "\n",
    "X, labels, true_w, true_b = generate_data()\n",
    "b = PerceptronClassifier()\n",
    "b.fit(X, labels, epochs=100)\n",
    "\n",
    "acc_rate = (((b.predict(X) * labels) > 0).mean())\n",
    "print(\"Training Accuracy Rate: {}\".format(acc_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True bias: -0.37\n",
      "True weights: [ 0.10988004]\n",
      "Current x: [ 5.00630542]\n",
      "Current bias: 2.00\n",
      "Current weights: [ 13.78474226]\n",
      "Current Score: [ 5.00630542] * [ 13.78474226] + 2.0 = 71.01062994176876 > 0\n",
      "Current Prediction: 1\n",
      "Current True Label: -1\n",
      "Error: True\n",
      "Update EQS:\n",
      "new_weights += (label*feature_vector)\n",
      "Weights Update: += (-1 * [ 5.00630542]) = [-5.00630542]\n",
      "new_bias += label\n",
      "Bias Update: += -1\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8IAAAGtCAYAAADd1s0LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npIFiISxJkiRJGij/H2BwYXPUcYMKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0fa8f3ef98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Button, Textarea\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "class PerceptronAnimator(object):\n",
    "    def __init__(self, X, y, true_w, true_b):\n",
    "        self.y_min = -1.1 \n",
    "        self.y_max = 1.1        \n",
    "        self.row_i = 0\n",
    "        self.advance_button = Button(description=\"Advance\")\n",
    "        self.advance_button.on_click(self.advance)\n",
    "        self.reverse_botton = Button(description=\"Reverse\")\n",
    "        self.reverse_botton.on_click(self.reverse)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.model = PerceptronClassifier\n",
    "        display(self.advance_button)\n",
    "        display(self.reverse_botton)\n",
    "        self.true_b = true_b\n",
    "        self.true_w = true_w\n",
    "        self.thresholds = []\n",
    "        self.errors = []\n",
    "        self.advance(self.advance_button)\n",
    "\n",
    "        \n",
    "    def get_data(self, row_i):\n",
    "        return self.X[:row_i], self.y[:row_i]\n",
    "    \n",
    "    def get_model(self, X, y):\n",
    "        model = self.model()\n",
    "        model.fit(X, y, epochs=1)\n",
    "        return model\n",
    "    \n",
    "    def make_plots(self, ):\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        ax1.set_ylim(self.y_min, self.y_max)\n",
    "        ax1.set_xlabel(\"X\")\n",
    "        ax1.set_ylabel(\"Predict(X) / Y\")\n",
    "        ax2.set_ylabel(\"Steps\")\n",
    "        ax2.set_xlabel(\"Weight Adjustment\")        \n",
    "        return ax1, ax2\n",
    "    \n",
    "    def advance(self, b):\n",
    "        ax1, ax2 = self.make_plots()\n",
    "        next_x, next_y, ax1 = self.display_next(ax1)        \n",
    "        threshold, error, ax1 = self.display_state(next_x, next_y, ax1)\n",
    "        self.row_i += 1\n",
    "        self.thresholds.append(threshold)\n",
    "        self.display_thresholds(ax1)\n",
    "        handles, labels = ax1.get_legend_handles_labels()\n",
    "        ax1.legend(handles, labels)        \n",
    "        self.errors.append(error)\n",
    "        self.plot_errors(ax2)\n",
    "        \n",
    "    def reverse(self, b):\n",
    "        ax1, ax2 = self.make_plots()\n",
    "        if self.row_i > 0:\n",
    "            self.row_i -= 1\n",
    "            x, y, ax1 = self.display_next(ax1)            \n",
    "            threshold, error, ax1 = self.display_state(x, y, ax1)\n",
    "             \n",
    "            self.thresholds = self.thresholds[:-1]\n",
    "            self.display_thresholds(ax1)\n",
    "            handles, labels = ax1.get_legend_handles_labels()\n",
    "            ax1.legend(handles, labels)            \n",
    "            self.errors = self.errors[:-1]\n",
    "            self.plot_errors(ax2)\n",
    "    def display_state(self, next_x, next_y, ax):\n",
    "        clear_output()\n",
    "\n",
    "        X, Y = self.get_data(self.row_i)\n",
    "        model = self.get_model(X, Y)\n",
    "        threshold = self.find_threshold(model)\n",
    "        color_direction = self.pick_color_direction(model, threshold[-1])\n",
    "        ax.scatter(X, Y)\n",
    "        x_, y_ = self.make_line(ax, model)   \n",
    "        ax.plot(x_, y_, label='current decision boundary')        \n",
    "\n",
    "        error = self.update_text_box(model, next_x, next_y) \n",
    "        plt.legend()\n",
    "        ax = self.shade_plot(threshold[-1], ax, color_direction)\n",
    "        \n",
    "        return threshold, error, ax\n",
    "\n",
    "    def display_thresholds(self, ax):\n",
    "        if len(self.thresholds) > 1:            \n",
    "            ax.axvline(self.thresholds[-2], color='green', linestyle=\"--\", label='last threshold value')              \n",
    "    def plot_errors(self, ax):\n",
    "        \n",
    "        for i, err in enumerate(self.errors):\n",
    "            ax.barh(i, err)\n",
    "        largest_bar = max(max(self.errors), abs(min(self.errors)))\n",
    "        ax.set_ylim(-1, len(self.errors) + 2)\n",
    "        ax.set_xlim(min(-1, -1*largest_bar), max(1, largest_bar))\n",
    "        ax.axvline(0)\n",
    "\n",
    "    def display_next(self, ax):\n",
    "        x, y = self.get_data(self.row_i +1)\n",
    "        model = self.get_model(x[:-1], y[:-1])\n",
    "        x, y = x[-1], y[-1]\n",
    "        correct = int(y) == int(model.predict(x))\n",
    "\n",
    "        #raise ValueError(\"\")\n",
    "        if correct:\n",
    "            ax.scatter(x, y, color='black', s=100.)\n",
    "        else:\n",
    "            ax.scatter(x, y, color='black', s=100., marker = 'X')\n",
    "        return x, y, ax\n",
    "        \n",
    "\n",
    "    \n",
    "    def shade_plot(self, threshold, ax, color_direction):\n",
    "        y = range(int(self.y_min - 1), int(self.y_max + 1))\n",
    "        min_x = min(ax.get_xticks())\n",
    "        max_x = max(ax.get_xticks())\n",
    "        if color_direction: \n",
    "            ax.fill_betweenx(y, threshold, max_x, color='green', alpha=.2)\n",
    "            ax.fill_betweenx(y, min_x, threshold, color='red', alpha=.2)\n",
    "        else:\n",
    "            ax.fill_betweenx(y, threshold, max_x, color='red', alpha=.2)\n",
    "            ax.fill_betweenx(y, min_x, threshold, color='green', alpha=.2)            \n",
    "        return ax\n",
    "        \n",
    "        \n",
    "    def update_text_box(self, model, next_x, next_y):\n",
    "        next_pred = model.predict(next_x)\n",
    "        if next_pred == 0:\n",
    "            next_pred = -1\n",
    "        error = next_pred * next_y < 0\n",
    "        update_b = next_y if error else 0\n",
    "        update_w = next_y * next_x if error else 0\n",
    "        msg = \"True bias: {:.2f}\".format(self.true_b)\n",
    "        msg += \"\\n\"\n",
    "        msg +=\"True weights: {}\".format(self.true_w)\n",
    "        msg += \"\\n\"              \n",
    "        msg += \"Current x: {}\".format(next_x)\n",
    "        msg += \"\\n\"        \n",
    "        msg += \"Current bias: {:.2f}\".format(model.bias)\n",
    "        msg += \"\\n\"\n",
    "        msg +=\"Current weights: {}\".format(model.weights) \n",
    "        msg += \"\\n\"\n",
    "        msg +=\"Current Score: {0} * {1} + {2} = {3} > 0\".format(next_x, model.weights, model.bias, model.predict_score(next_x)) \n",
    "        msg += \"\\n\"        \n",
    "        msg += \"Current Prediction: {}\".format(next_pred)        \n",
    "        msg += \"\\n\"\n",
    "        msg += \"Current True Label: {}\".format(int(next_y))        \n",
    "        msg += \"\\n\"        \n",
    "        msg += \"Error: {}\".format(error)\n",
    "        msg += \"\\n\"\n",
    "        if next_pred != int(next_y):\n",
    "            msg += \"Update EQS:\"\n",
    "            msg += \"\\n\" \n",
    "            msg += \"new_weights += (label*feature_vector)\"\n",
    "            msg += \"\\n\"        \n",
    "            msg += \"Weights Update: += ({0} * {1}) = {2}\".format(int(next_y), next_x, update_w)            \n",
    "            msg += \"\\n\" \n",
    "            msg += \"new_bias += label\"\n",
    "            msg += \"\\n\"             \n",
    "            msg += \"Bias Update: += {}\".format(int(next_y))\n",
    "            msg += \"\\n\"        \n",
    "        print(msg)\n",
    "        return update_w\n",
    "        \n",
    "        \n",
    "    def make_line(self, ax, model):\n",
    "        x = [i / 10. for i in range(-300, 300)]\n",
    "        y = np.array([(model.bias + i * model.weights[0]) > 0 for i in x]).astype(float)\n",
    "        y[np.where(y==0)] = -1\n",
    "        return x, y\n",
    "    \n",
    "    def find_threshold(self, model):\n",
    "        x_range = np.array([i/10. for i in range(-300, 300)])[:, np.newaxis]\n",
    "        y = abs(model.predict_score(x_range))\n",
    "        \n",
    "        idx = abs(y).argmin()\n",
    "        return x_range[idx]\n",
    "    \n",
    "    def pick_color_direction(self, model, threshold):\n",
    "        if model.predict(threshold + 1) > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "X, labels, true_w, true_b = generate_data()\n",
    "p = PerceptronAnimator(X, labels, true_w, true_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What/how the model learns is sensitive to the ordering of the Xs. If X is multidimensional, and the last value is an outlier, our updates can be large, throwing the weights out of wack.\n",
    "\n",
    "Solution: averaging the weights over an epoch, so that weights **that usually work, and thus stick around** are used.\n",
    "\n",
    "##### Averaged Perceptron Learning:\n",
    "```\n",
    "list_of_weights, list_of_biases = list(), list()\n",
    "For each training epoch:\n",
    "    For each feature_vector, label:\n",
    "        prediction = weights * feature_vector + bias\n",
    "\n",
    "        if sign(label) != sign(prediction):\n",
    "            weights = weights + (label*feature_vector)\n",
    "            bias = bias + label\n",
    "        add weights to list_of_weights\n",
    "        add bias to list_of_biases\n",
    "shuffle data\n",
    "```\n",
    "\n",
    "##### Averaged Perceptron Prediction:\n",
    "```\n",
    "weights = average(list_of_weights)\n",
    "bias = average(list_of_bias)\n",
    "\n",
    "prediction = weights * feature_vector + bias > 0\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of the Averaged Perceptron Model:\n",
    "* Can only learn linear decision boundaries\n",
    "* Can only use hardcoded representations\n",
    "    * Solution: neural network.\n",
    "    * Coming in SpaCy 2.0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='qacode'></a>\n",
    "### Example Rule Based QA Component Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_answer_requirements(token):\n",
    "    if token.tag_ == 'WRB':\n",
    "        if token.lower_ == 'where':\n",
    "            #Where was Star Wars Filmed\n",
    "            return ['LOCATION']\n",
    "        elif token.lower_ == 'when':\n",
    "            #When was Star Wars Filmed\n",
    "            return ['DATE']\n",
    "        elif token.lower_ == 'how':\n",
    "            #How much did Star Wars make?\n",
    "            if token.nbor().lower_ in ('much', 'many'):\n",
    "                return ['QUANTITY']\n",
    "\n",
    "            #How old is star wars?\n",
    "            elif token.nbor().lower_ in ('long', 'old'):\n",
    "                return ['DURATION']\n",
    "            else:\n",
    "                return False\n",
    "        elif token.lower() == 'whom':\n",
    "            #Whom did you see?\n",
    "            return ['PERSON','ORG']      \n",
    "        else:\n",
    "            return False\n",
    "    elif token.tag_ == 'WP':\n",
    "        #Asking for Identity\n",
    "        if token.lower_ in ('who', 'whose'):\n",
    "            #Who directed Star Wars?\n",
    "            return ['PERSON','ORG']\n",
    "        if token.lower_ in ('which','what'):\n",
    "            #What is Star Wars\n",
    "            return False \n",
    "        else: \n",
    "            return False\n",
    "    elif token.tag_ == 'WDT':\n",
    "        #asking for a choice among options\n",
    "        if token.lower_ in ('which','what'):\n",
    "            #which Star Wars did you like best?\n",
    "            return [token.nbor().lower_] #return neighbor\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['LOCATION'], False, False, False, False, False]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(get_answer_requirements, nlp('Where was Star Wars filmed?')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PERSON', 'ORG'], False, False, False, False]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(get_answer_requirements, nlp('Who directed Star Wars?')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['movie'], False, False, False, False, False]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(get_answer_requirements, nlp('Which movie was first released?')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, ['country'], False, False, False, False, False, False]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(get_answer_requirements, nlp('In which country was Star Wars filmed?')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#applications'>back</a>\n",
    "<a name=\"wordsense\"></a>\n",
    "##### Word sense disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Synset('shower.n.01') a plumbing fixture that sprays water over you\n",
      "Synset('shower.n.02') washing yourself by standing upright under water sprayed from a nozzle\n",
      "Synset('shower.n.03') a brief period of precipitation\n",
      "Synset('shower.n.04') a sudden downpour (as of tears or sparks etc) likened to a rain shower\n",
      "Synset('exhibitor.n.01') someone who organizes an exhibit for others to see\n",
      "Synset('shower.n.06') a party of friends assembled to present gifts (usually of a specified kind) to a person\n",
      "Synset('lavish.v.01') expend profusely; also used with abstract nouns\n",
      "Synset('shower.v.02') spray or sprinkle with\n",
      "Synset('shower.v.03') take a shower; wash one's body in the shower\n",
      "Synset('shower.v.04') rain abundantly\n",
      "Synset('shower.v.05') provide abundantly with\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "for syn in wn.synsets('shower'):\n",
    "    print(syn, syn.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#applications'>back</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
